{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45914d66-a7de-49be-822e-e45828f884d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19a4eb8-3c7d-41d2-812f-1217366072b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory structure\n",
    "OUTPUT_DIR = './intrusion_detection_model'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5211bf56-9ec8-49fa-ade4-ed6a62556956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34397394-5849-4fb4-8119-74cce110c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Data loaded in 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Loading and exploring the dataset\n",
    "train_path = 'UNSW_NB15/UNSW_NB15_training-set.csv'\n",
    "test_path = 'UNSW_NB15/UNSW_NB15_testing-set.csv'\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "start_time = time.time()\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2929cf-0a45-4634-b89b-c6769ff8e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (257673, 45)\n",
      "Target column values (attack_cat): ['Normal' 'Reconnaissance' 'Backdoor' 'DoS' 'Exploits' 'Analysis'\n",
      " 'Fuzzers' 'Worms' 'Shellcode' 'Generic']\n",
      "Target column values (label): [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"Target column values (attack_cat):\", df['attack_cat'].unique())\n",
    "print(\"Target column values (label):\", df['label'].unique())  # 0: Normal, 1: Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138f4d2d-8b65-40eb-838a-f124e6dd434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing data...\n",
      "Explicitly dropping ID column...\n",
      "\n",
      "Attack Category Mapping:\n",
      "Analysis: 0\n",
      "Backdoor: 1\n",
      "DoS: 2\n",
      "Exploits: 3\n",
      "Fuzzers: 4\n",
      "Generic: 5\n",
      "Normal: 6\n",
      "Reconnaissance: 7\n",
      "Shellcode: 8\n",
      "Worms: 9\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess the data\n",
    "print(\"\\nPreprocessing data...\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Explicitly drop the ID column to prevent it from being used in modeling\n",
    "print(\"Explicitly dropping ID column...\")\n",
    "df.drop(columns=['id'], inplace=True, errors='ignore')\n",
    "\n",
    "# Store categorical columns for one-hot encoding instead of dropping them\n",
    "categorical_cols = ['proto', 'service', 'state']\n",
    "\n",
    "# Make a copy of categorical features before transformation for later use\n",
    "cat_data = df[categorical_cols].copy()\n",
    "\n",
    "# Encode 'attack_cat' (multiclass target) & prepare binary target\n",
    "label_encoder = LabelEncoder()\n",
    "df['attack_cat_encoded'] = label_encoder.fit_transform(df['attack_cat'])\n",
    "df['label'] = df['label'].astype(int)  # Ensure binary label is int\n",
    "\n",
    "# Store the class mappings for later use\n",
    "attack_cat_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "print(\"\\nAttack Category Mapping:\")\n",
    "for cat, code in attack_cat_mapping.items():\n",
    "    print(f\"{cat}: {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07bf11c3-1d7e-4743-8f59-e196ffc42bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: 93000\n",
      "Reconnaissance: 13987\n",
      "Backdoor: 2329\n",
      "DoS: 16353\n",
      "Exploits: 44525\n",
      "Analysis: 2677\n",
      "Fuzzers: 24246\n",
      "Worms: 174\n",
      "Shellcode: 1511\n",
      "Generic: 58871\n"
     ]
    }
   ],
   "source": [
    "for category in df['attack_cat'].unique():\n",
    "    count = df['attack_cat'].value_counts().get(category, 0)\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144a878b-9912-445b-a5d8-2d908448cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "minority attack class samples: 6691\n",
      "majority Attack class samples: 157982\n",
      "Class imbalance ratio (attack:normal): 15.74\n"
     ]
    }
   ],
   "source": [
    "# Split the data by class\n",
    "\n",
    "attack_data = df[df['label'] == 1]\n",
    "\n",
    "min_classes = ['Worms', 'Shellcode', 'Backdoor', 'Analysis']\n",
    "min_att_data = attack_data[attack_data['attack_cat'].isin(min_classes)]\n",
    "maj_att_data = attack_data[~attack_data['attack_cat'].isin(min_classes)]\n",
    "print(f\"\\nminority attack class samples: {len(min_att_data)}\")\n",
    "print(f\"majority Attack class samples: {len(maj_att_data)}\")\n",
    "\n",
    "# Print class imbalance ratio\n",
    "imbalance_ratio = len(maj_att_data) / len(min_att_data) * 4/6\n",
    "print(f\"Class imbalance ratio (attack:normal): {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641a2e27-208d-4d80-92bf-604b0f47abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying one-hot encoding to categorical features...\n",
      "Normalizing features...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# One-hot encode categorical features\n",
    "print(\"\\nApplying one-hot encoding to categorical features...\")\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Keep 'attack_cat_encoded' as the conditional label\n",
    "# Drop only columns that are not needed for CGAN input\n",
    "columns_to_drop = ['attack_cat', 'label']  # Keep 'attack_cat_encoded'\n",
    "df_encoded.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Normalize feature columns (excluding 'attack_cat_encoded')\n",
    "# as 'attack_cat_encoded' will be used as label in cgan\n",
    "print(\"Normalizing features...\")\n",
    "feature_cols = df_encoded.columns[df_encoded.columns != 'attack_cat_encoded']\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[feature_cols] = scaler.fit_transform(df_encoded[feature_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9c9d5b-4238-42d7-a249-3d03e95574a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test splitting done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "df_ids_original = df_encoded.copy()\n",
    "X_orig = df_ids_original.drop(columns=['attack_cat_encoded']).values\n",
    "y_orig = df_ids_original['attack_cat_encoded'].values\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "                                                        X_orig, y_orig, test_size=0.3,\n",
    "                                                        random_state=42, stratify=y_orig)\n",
    "print(\"train test splitting done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c13a3e-b2a3-4961-bc57-cce47bda42b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest on original data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest ---\n",
    "print(\"\\nTraining Random Forest on original data...\")\n",
    "rf_orig = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_orig.fit(X_train_orig, y_train_orig)\n",
    "y_pred_rf_orig = rf_orig.predict(X_test_orig)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d568072-f8b9-49de-baf0-360e90ec2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest on original data: 0.8257\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate accuracy\n",
    "accuracy_rf_orig = accuracy_score(y_test_orig, y_pred_rf_orig)\n",
    "print(f\"Accuracy of Random Forest on original data: {accuracy_rf_orig:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5202a57-e63c-45a2-9210-5a10f804067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on original data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# --- XGBoost ---\n",
    "print(\"Training XGBoost on original data...\")\n",
    "xgb_orig = XGBClassifier(eval_metric='mlogloss')\n",
    "xgb_orig.fit(X_train_orig, y_train_orig)\n",
    "y_pred_xgb_orig = xgb_orig.predict(X_test_orig)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382e1511-808b-491c-8178-aae5ab3d5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGB on original data: 0.8331\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_rf_orig = accuracy_score(y_test_orig, y_pred_xgb_orig)\n",
    "print(f\"Accuracy of XGB on original data: {accuracy_rf_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0f5e750-4bbb-4093-9b7a-8d0398a7f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, optimizers\n",
    "\n",
    "class CGAN:\n",
    "    def __init__(self, noise_dim, num_classes, feature_dim, lr=0.0002):\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        # Build models\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # Optimizers\n",
    "        self.gen_optimizer = optimizers.Adam(self.lr, beta_1=0.5)\n",
    "        self.disc_optimizer = optimizers.Adam(self.lr, beta_1=0.5)\n",
    "\n",
    "        # Loss\n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_input = layers.Input(shape=(self.noise_dim,))\n",
    "        label_input = layers.Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = layers.Embedding(self.num_classes, self.noise_dim)(label_input)\n",
    "        label_embedding = layers.Flatten()(label_embedding)\n",
    "\n",
    "        x = layers.multiply([noise_input, label_embedding])\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dense(self.feature_dim, activation='sigmoid')(x)\n",
    "\n",
    "        return models.Model([noise_input, label_input], x, name=\"Generator\")\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        data_input = layers.Input(shape=(self.feature_dim,))\n",
    "        label_input = layers.Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = layers.Embedding(self.num_classes, self.feature_dim)(label_input)\n",
    "        label_embedding = layers.Flatten()(label_embedding)\n",
    "\n",
    "        x = layers.multiply([data_input, label_embedding])\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dense(1)(x)\n",
    "\n",
    "        return models.Model([data_input, label_input], x, name=\"Discriminator\")\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    def train(self, X_real, y_real, batch_size=64, epochs=200):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_real, y_real)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "        print(\"training started : \\n\")\n",
    "        for epoch in range(epochs):\n",
    "            for real_batch, label_batch in dataset:\n",
    "                # Generate noise and fake labels\n",
    "                noise = tf.random.normal([real_batch.shape[0], self.noise_dim])\n",
    "                generated_data = self.generator([noise, label_batch], training=True)\n",
    "\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    # Discriminator loss\n",
    "                    real_output = self.discriminator([real_batch, label_batch], training=True)\n",
    "                    fake_output = self.discriminator([generated_data, label_batch], training=True)\n",
    "                    d_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                    # Generator loss\n",
    "                    noise = tf.random.normal([real_batch.shape[0], self.noise_dim])\n",
    "                    generated_data = self.generator([noise, label_batch], training=True)\n",
    "                    fake_output = self.discriminator([generated_data, label_batch], training=True)\n",
    "                    g_loss = self.generator_loss(fake_output)\n",
    "\n",
    "                # Backpropagation\n",
    "                gradients_of_generator = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "                gradients_of_discriminator = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                self.gen_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "                self.disc_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "            # Epoch Summary\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss:.4f} | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "    def generate_samples(self, num_samples, class_label):\n",
    "        noise = tf.random.normal([num_samples, self.noise_dim])\n",
    "        labels = tf.convert_to_tensor([class_label] * num_samples)\n",
    "        synthetic_data = self.generator([noise, labels], training=False)\n",
    "        return synthetic_data.numpy()\n",
    "    def save(self, save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.generator.save(os.path.join(save_dir, \"generator.h5\"))\n",
    "        self.discriminator.save(os.path.join(save_dir, \"discriminator.h5\"))\n",
    "        print(f\"‚úÖ CGAN models saved to: {save_dir}\")\n",
    "\n",
    "    def load(self, save_dir):\n",
    "        self.generator = tf.keras.models.load_model(os.path.join(save_dir, \"generator.h5\"))\n",
    "        self.discriminator = tf.keras.models.load_model(os.path.join(save_dir, \"discriminator.h5\"))\n",
    "        print(f\"‚úÖ CGAN models loaded from: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "006e4749-1f37-4a38-9673-2baadfa3978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples selected for CGAN training (classes: ['Worms', 'Shellcode', 'Backdoor', 'Analysis']): 6691\n",
      "Feature vector size: 196\n",
      "Unique class labels in GAN data: [0 1 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Filter for selected minority classes\n",
    "minority_classes = ['Worms', 'Shellcode', 'Backdoor','Analysis']\n",
    "minority_codes = [attack_cat_mapping[cls] for cls in minority_classes]\n",
    "gan_data = df_encoded[df['attack_cat_encoded'].isin(minority_codes)].copy()\n",
    "\n",
    "# Extract features (X) and conditional labels (y) for CGAN\n",
    "X_gan = gan_data.drop(columns=['attack_cat_encoded']).values\n",
    "y_gan = gan_data['attack_cat_encoded'].values\n",
    "\n",
    "print(f\"\\nSamples selected for CGAN training (classes: {minority_classes}): {X_gan.shape[0]}\")\n",
    "print(f\"Feature vector size: {X_gan.shape[1]}\")\n",
    "print(f\"Unique class labels in GAN data: {np.unique(y_gan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60af46fa-94d9-4804-a5ed-44c5e3d9e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started : \n",
      "\n",
      "Epoch 1/300 | D Loss: 1.0046 | G Loss: 0.9632\n",
      "Epoch 10/300 | D Loss: 0.4786 | G Loss: 1.7295\n",
      "Epoch 20/300 | D Loss: 0.0657 | G Loss: 3.4465\n",
      "Epoch 30/300 | D Loss: 0.3489 | G Loss: 2.3746\n",
      "Epoch 40/300 | D Loss: 0.1160 | G Loss: 4.1543\n",
      "Epoch 50/300 | D Loss: 0.0459 | G Loss: 5.9137\n",
      "Epoch 60/300 | D Loss: 0.0055 | G Loss: 6.3847\n",
      "Epoch 70/300 | D Loss: 0.1969 | G Loss: 2.1667\n",
      "Epoch 80/300 | D Loss: 0.0356 | G Loss: 3.7837\n",
      "Epoch 90/300 | D Loss: 0.0844 | G Loss: 4.9143\n",
      "Epoch 100/300 | D Loss: 0.0099 | G Loss: 6.0849\n",
      "Epoch 110/300 | D Loss: 0.0069 | G Loss: 5.3787\n",
      "Epoch 120/300 | D Loss: 0.0797 | G Loss: 4.6612\n",
      "Epoch 130/300 | D Loss: 0.0077 | G Loss: 5.5488\n",
      "Epoch 140/300 | D Loss: 0.0028 | G Loss: 6.5191\n",
      "Epoch 150/300 | D Loss: 0.0019 | G Loss: 6.7708\n",
      "Epoch 160/300 | D Loss: 0.1540 | G Loss: 7.4478\n",
      "Epoch 170/300 | D Loss: 0.0021 | G Loss: 7.5059\n",
      "Epoch 180/300 | D Loss: 0.3875 | G Loss: 2.0460\n",
      "Epoch 190/300 | D Loss: 0.2121 | G Loss: 3.8494\n",
      "Epoch 200/300 | D Loss: 0.0180 | G Loss: 6.7854\n",
      "Epoch 210/300 | D Loss: 0.3022 | G Loss: 4.5045\n",
      "Epoch 220/300 | D Loss: 0.4039 | G Loss: 4.4280\n",
      "Epoch 230/300 | D Loss: 0.3432 | G Loss: 3.5062\n",
      "Epoch 240/300 | D Loss: 0.2459 | G Loss: 4.5801\n",
      "Epoch 250/300 | D Loss: 0.8959 | G Loss: 4.3977\n",
      "Epoch 260/300 | D Loss: 0.4477 | G Loss: 3.2029\n",
      "Epoch 270/300 | D Loss: 0.2426 | G Loss: 4.5349\n",
      "Epoch 280/300 | D Loss: 0.0560 | G Loss: 3.9183\n",
      "Epoch 290/300 | D Loss: 0.0321 | G Loss: 4.2983\n",
      "Epoch 300/300 | D Loss: 0.6914 | G Loss: 3.6095\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "# Initialize CGAN\n",
    "cgan = CGAN(\n",
    "    noise_dim=100,\n",
    "    num_classes=len(attack_cat_mapping),\n",
    "    feature_dim=X_gan.shape[1],\n",
    "    lr=0.0002\n",
    ")\n",
    "# Train CGAN\n",
    "cgan.train(X_real=X_gan, y_real=y_gan, batch_size=64, epochs=300)\n",
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40ed6a39-2952-4ea1-afbf-b00557aa55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples per minority class\n",
    "num_synthetic = 3000\n",
    "X_synth_list = []\n",
    "y_synth_list = []\n",
    "\n",
    "# Generate synthetic samples for each minority class\n",
    "for cls_name in ['Worms', 'Shellcode', 'Backdoor','Analysis']:\n",
    "    class_code = attack_cat_mapping[cls_name]\n",
    "    X_synth = cgan.generate_samples(num_synthetic, class_code)\n",
    "    y_synth = np.array([class_code] * num_synthetic)\n",
    "    \n",
    "    X_synth_list.append(X_synth)\n",
    "    y_synth_list.append(y_synth)\n",
    "\n",
    "# Combine all synthetic data\n",
    "X_synth_total = np.vstack(X_synth_list)\n",
    "y_synth_total = np.hstack(y_synth_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fd23154-cf5e-4566-ba19-2a77eea8e04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test splitting for augmented done\n"
     ]
    }
   ],
   "source": [
    "X_augmented = np.vstack([X_orig, X_synth_total])\n",
    "y_augmented = np.hstack([y_orig, y_synth_total])\n",
    "\n",
    "# Train/test split (same proportion)\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "                                                   X_augmented, y_augmented, test_size=0.3,\n",
    "                                                    random_state=42, stratify=y_augmented)\n",
    "print(\"train test splitting for augmented done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4bae703-aae6-47e3-93a2-e1851fa0e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training on: Original (Imbalanced)\n",
      "\n",
      "üîç Random Forest on Original (Imbalanced) data\n",
      "Accuracy: 0.8257 | F1-score: 0.8173\n",
      "\n",
      "üîç XGBoost on Original (Imbalanced) data\n",
      "Accuracy: 0.8331 | F1-score: 0.8162\n",
      "\n",
      "üìä Training on: GAN-Augmented\n",
      "\n",
      "üîç Random Forest on GAN-Augmented data\n",
      "Accuracy: 0.8327 | F1-score: 0.8289\n",
      "\n",
      "üîç XGBoost on GAN-Augmented data\n",
      "Accuracy: 0.8417 | F1-score: 0.8302\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, label):\n",
    "    print(f\"\\nüìä Training on: {label}\")\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    for model_name, y_pred in zip([\"Random Forest\", \"XGBoost\"], [y_pred_rf, y_pred_xgb]):\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        # cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nüîç {model_name} on {label} data\")\n",
    "        print(f\"Accuracy: {acc:.4f} | F1-score: {f1:.4f}\")\n",
    "        # print(\"Confusion Matrix:\")\n",
    "        # print(cm)\n",
    "\n",
    "# Evaluate on original and augmented datasets\n",
    "train_and_evaluate(X_train_orig, X_test_orig, y_train_orig, y_test_orig, label=\"Original (Imbalanced)\")\n",
    "train_and_evaluate(X_train_aug, X_test_aug, y_train_aug, y_test_aug, label=\"GAN-Augmented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4f3a6f5-b7c8-48df-8939-fd2117de69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_SAVE_PATH = os.path.join(OUTPUT_DIR, 'models', 'cgan')\n",
    "os.makedirs(GAN_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save Generator\n",
    "cgan.generator.save(os.path.join(GAN_SAVE_PATH, 'generator.keras'))\n",
    "\n",
    "# Save Discriminator\n",
    "cgan.discriminator.save(os.path.join(GAN_SAVE_PATH, 'discriminator.keras'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03a816-baff-4e50-b5ea-5872542e1e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
